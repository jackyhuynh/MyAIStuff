---
title: "Summary of Decision Tree and Random Forest in Practice"
author: "Truc Huynh"
date: "11/27/2020"
output: 
  word_document:
    toc: yes
    toc_depth: '5'
  html_document:
    toc: yes
    toc_depth: '5'
    df_print: paged
    code_folding: hide
  pdf_document:
    toc: yes
    toc_depth: 5
    df_print: kable
code_download: yes
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
library (DescTools)
```
# Part 0: Code Practice
## Decision Trees with Package party

build a decision tree for the iris data with function ctree() in package party [Hothorn et al., 2010].

```{r exploration, warning=FALSE }
str(iris)
set.seed(1234)
ind<- sample(2,nrow(iris), replace = TRUE, prob=c(0.7,0.3))
trainData <- iris[ind==1,]
testData<- iris[ind==2,]

# Use diffrent setting to build a decision Tree.
library(party)
myFormula <- Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width
iris_ctree <- ctree(myFormula, data=trainData)

#Check the prediction:
table(predict(iris_ctree), trainData$Species)

print(iris_ctree)
```

```{r treePlot, warning=FALSE}
plot(iris_ctree)
plot(iris_ctree, type="simple")

# predict on test data
testPred <- predict(iris_ctree, newdata = testData)
table(testPred, testData$Species)

```
## Decision Trees with Package rpart
```{r packageRpart, warning=FALSE}
data("bodyfat", package = "TH.data")
dim(bodyfat)
attributes(bodyfat)

bodyfat[1:5,]
set.seed(1234)
ind <- sample(2, nrow(bodyfat), replace=TRUE, prob=c(0.7, 0.3))
bodyfat.train <- bodyfat[ind==1,]
bodyfat.test <- bodyfat[ind==2,]
# train a decision tree
library(rpart)
myFormula <- DEXfat ~ age + waistcirc + hipcirc + elbowbreadth + kneebreadth
bodyfat_rpart <- rpart(myFormula, data = bodyfat.train,control = rpart.control(minsplit = 10))
attributes(bodyfat_rpart)
```

```{r rpartTable2}
print(bodyfat_rpart$cptable)
print(bodyfat_rpart)
plot(bodyfat_rpart)
text(bodyfat_rpart, use.n=T)

```


```{r rpartTable3}
opt <- which.min(bodyfat_rpart$cptable[,"xerror"])
cp <- bodyfat_rpart$cptable[opt, "CP"]
bodyfat_prune <- prune(bodyfat_rpart, cp = cp)
print(bodyfat_prune)
plot(bodyfat_prune)
text(bodyfat_prune, use.n=T)
```

The predictions of a good model are expected to be equal or very close to their actual values, that is, most points should be on or close to the diagonal line.
```{r rpartTable4}
DEXfat_pred <- predict(bodyfat_prune, newdata=bodyfat.test)
xlim <- range(bodyfat$DEXfat)
plot(DEXfat_pred ~ DEXfat, data=bodyfat.test, xlab="Observed", ylab="Predicted", ylim=xlim, xlim=xlim)
abline(a=0, b=1)

```
</br>

## Random Forest
Again, the iris data is first split into two subsets: training (70%) and test (30%).
```{r randomForest}
ind <- sample(2, nrow(iris), replace=TRUE, prob=c(0.7, 0.3))
trainData <- iris[ind==1,]
testData <- iris[ind==2,]

```
the formula is set to “Species ∼ .”, which means to predict Species with all other variables in the data.
```{r randomForest1, warning=FALSE}
library(randomForest)
rf <- randomForest(Species ~ ., data=trainData, ntree=100, proximity=TRUE)
table(predict(rf), trainData$Species)
print(rf)
attributes(rf)
plot(rf)
```

```{r randomForest2}
importance(rf)
 varImpPlot(rf)
 irisPred <- predict(rf, newdata=testData)
 table(irisPred, testData$Species)
plot(margin(rf, testData$Species))
```
</br>

# Part I: Reading
Read  V. Chandola,  A. Banerjee and V. Kumar,  “Outlier Detection: A Survey”, ACM Computing Surveys Vol.41, no. 3.  2009. 
Answer the following questions: 
</br>

## Summarize three main outlier detection techniques 
- (2.2.1) Supervised outlier detection techniques: Assume the training dataset contains normal and outlier class. The normal approach is to build predictive models for both normal and abnormal classes; data will be compared with both models to see where it belongs. Pros: techniques can build accurate models. Cons: labeled training data might be prohibitively expensive to obtain since it requires a lot of human effort to obtain the labeled training data set.
- (2.2.2) Semi-supervised outlier detection techniques: Assume the availability of only one class. The normal approach is to model only the available class and decare any test instance which does not fit this model to belong to the other class. The reason is very hard to obtain all the possible outlying behaviors that can occur in the training dataset. The techniques which model only the normal instances are very popular since they are easy to obtain. Also, normal behavior is easier to construct than abnormal.
- (2.2.3) Unsupervised outlier detection techniques: These techniques in this category does not make any assumption about label training, and it focus on making assumption about the data. For examples, normal instances are far more frequent than outliers. Thus a frequently occurring pattern is typically considered normal while a rare occurrence is an outlier. 
Unsupervised outlier detection techniques and semi-supervised outlier detection techniques are widely used than supervised outlier detection techniques.
</br>

##	Summarize different types of outliers 
- (2.3.1) Type I outliers: A data instance is an outlier due to its attribute values which are inconsistent with values taken by normal instances. Techniques that detect Type I outliers analyze the relation of an individual instance with respect to rest of the data instances.
- (2.3.2) Type II outliers : 
These outliers are caused due to the occurrence of an individual data instance in a specific context in the given data. Type II outliers are defined with respect to a context. In fact, a data instance may be a Type II outlier in a specific context (fall under some conditions); but its identical data instance (in terms of behavioral attributes) could be considered normal in a different context. Mostly explored in time-series data, and spatial data. For example, a credit card user usually spends $ 20 at a gas station and $ 200 at a jewelry store. If he spends $ 200 at someday, it considers a type II outliers.
- (2.3.3) Type III outliers: 
When a subset of data instances is outlying with respect to the entire data set, it is considered type III outliers. Its instance data is not outlying by itself, but their occurrences is the whole structure is anomalous. For example, normal shopping pattern of a credit card user is that the gas station, the grocery store, and the restaurant. Three of more purchase at a gas station on the same day (at different time or under more conditions) is potential card theft. This sequence of transactions is a Type III outlier.
Type I outliers may be detected in any type of data. Type II and Type III outliers require the presence of sequential or spatial structure in the data. But the nature of outliers actually required to be detected by a particular outlier detection algorithm is determined by the specific problem formulation. 
</br>

##	Summarize two strategies for evaluation of outlier detection techniques
-	(15.1) Detecting outliers in a given data set:
A labeling type of outlier detection technique is typically evaluated using any of the evaluation techniques from 2-class classification literature. In the following steps:
<ol>
<li>First, choose the dataset.<li>
<li>Second, split the data set into training and test (if invoke training).<li>
<li>Third, use the training dataset to train the predicted model.<li>
<li>Fourth, applying the outlier detection techniques to the test dataset to detect the outliers and normal.<li>
<li>Finally, Use the predicted label compare with the actual labels to construct a confusion matrix.<li>
</ol>
- Conclusion: here are many evaluation metrics such as precision, recall, accuracy, false positive rate, false negative rate, detection rates, ROC-curve… have been applied in outlier detection literature. In fact, the choice of an evaluation data set depends on what type of data is the outlier detection technique targeted for. The UCI KDD archive contain many benchmark data sets that use to evaluate outlier algorithm. However, most of them are only available for Type I outlier detection techniques; some can be used for Type II; none for type III.
-	(15.2) Evaluation in the application domain : Outlier detection is a highly application domain-oriented concept, and there is no techniques can be applied for all possible setting. One approach is to choose a dataset (sample) belong to the target application domain. Evaluation measures the performance of the entire outlier detection setting (including the technique, the features chosen and other related parameters/assumptions). It is also possible that evaluation technique that perform well in subsection, might not perform well in the application domain. But such evaluation is necessary to establish the usefulness of the technique in the target application domain. Challenges are benchmark data sets are not available publicly, and evaluation from application domain perspective is that labeled validation data is often not available at all.

</br>

# Part II: Outlier Analysis
## Question 1:
-	Create a histogram of the data distribution. Using R Studio, and dplyr package to draw the histogram. 
```{r histogram}
library(dplyr)

# Import data
A <- c(1, 3, 2, 1, 3, 2, 75, 1, 3, 2, 2, 1, 2, 3, 2, 1)
View (A)

library(ggplot2)
hist(A)
# Outlier has the value of 75
```
- Answer: Outlier has the value of 75.</br>

## Question 2:
- Compute the Z-value of each data point:
```{r zValue}
# 2.	Compute the Z-value of each data point.
# Which of these values can be considered the most extreme value?
Data <- data.frame(A)

Data %>% 
  mutate(zscore = (A- mean(A))/sd(A))
```
- Answer: Index 7(75) and z-score is 3.7466579 can be considered the most extreme value.</br>

## Question 3:
- Determine the nearest neighbor (k=1):
```{r nearest neighbor}
# 3.	Determine the nearest neighbor (k=1) of each data point. 
# Which data points have the largest value of the nearest neighbor distance?  
# Are they the correct outliers? 
library(FNN)
get.knn(Data, k=1,algorithm=c("brute"))

# The data show that value with index [7] have the largest value of the nearest 
# neighbor distance. Thus it is a outlier.
```
- Answer: The data show that value with index [7] have the largest value of the nearest neighbor distance. Thus it is a outlier.</br>

## Question 4:
```{r kMeans}
# 4.	Apply a k-means clustering algorithm (k=2) to the data set.
# Which data points lie furthest from the two means (the centroids of the two 
# clusters) found? Are they the correct outliers? 
clusters <- kmeans(Data[,1], 2)
str(clusters)
Data$Borough <- as.factor(clusters$cluster)
clusters[["centers"]]

```
- Answer: Data points lie furthest from the two means (the centroids of the two clusters) found:  There is two cluster found. However, the 75 is only contain 1 value (itself). Therefore, the value 75 itself is outlier.

## Question 5:
- Answer: There is no node that consider outlier because all the node is connect to point 1. 

# Part III: Classification
## Question 1-6:
- On paper.

## Question 7-11:
- Import the data for Classification
```{r classification }
train.data <- read.csv("~/R/DataMining/Decision_Tree/DATA.csv")
train.data $Instance <-NULL
train.data$a3 <- factor(train.data$a3)
train.data $class <- factor(train.data$class)
train.data 
```

- Calculate the entropy using R
```{r entropy}
Entropy(train.data$a1,train.data$class)
Entropy(train.data$a2,train.data$class)
Entropy(train.data$a3,train.data$class)
```
## Question 8
- Import the test data
```{r test.data}
# Create test data frame to test the data
a1<- c(TRUE,TRUE,TRUE,FALSE,FALSE,FALSE)
a2<- c(1,6,5,4,7,3)
a3<- c('A','A','B','B','C','C')
class<-c('+','+','-','+','-','-')
test.data<-data.frame(a1,a2,a3,class)
test.data$a2 <- as.integer(test.data$a2)
test.data$a3 <- factor(test.data$a3)
test.data$class <- factor(test.data$class)
test.data
```
- Training Model
```{r small.decision.tree}
# Training Model
myFormula <- class ~ a1+a2+a3
small_ctree <- ctree(myFormula, data=train.data)

# Check the prediction:
table(predict(small_ctree), train.data$class)

print(small_ctree)

# Draw the plot for the final tree generated by the decision tree classifier
plot(small_ctree)
plot(small_ctree, type="simple")

# predict on test data
testPred <- predict(small_ctree, newdata = test.data)

# test the model prediction
table(testPred, test.data$class)
```
## Question 9,10,11:
```{r rf.small.tree}
rf <- randomForest(class ~ ., data=train.data, ntree=50, proximity=TRUE)
table(predict(rf), train.data$class)
print(rf)
attributes(rf)
plot(rf)
```
