---
title: "WholeSaleCustomer"
author: "Truc Huynh"
date: "11/16/2020"
output:
  word_document:
    toc: yes
    toc_depth: '5'
  html_document:
    toc: yes
    toc_depth: '5'
    df_print: paged
    code_folding: hide
  pdf_document:
    toc: yes
    toc_depth: 5
    df_print: kable
code_download: yes
---
## Goal:
The goal is to segment the clients of a wholesale distributor based on their annual pending on diverse product categories.
For the data analysis, I am going to use R and R markdown.
```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries,include=TRUE, warning=FALSE}
#library to use for the analysis
library(psych)
library(e1071)
library(rpart)
library(rpart.plot)
library(caTools)
library(readr)
library(caret)
library(RColorBrewer)
library(fpc)
```
## Data Description
Download a customer data (named with Wholesale customers data.csv). For the detail of the dataset, refer to http://archive.ics.uci.edu/ml/datasets/Wholesale+customers#
The data includes the annual spending in monetary units on diverse product categories.There are 8 attributes. Two attributes, CHANNEL and REGION, are nominal, and the others are continuous.
(1) FRESH: annual spending (m.u.) on fresh products (Continuous);
(2) MILK: annual spending (m.u.) on milk products (Continuous);
(3) GROCERY: annual spending (m.u.) on grocery products (Continuous);
(4) FROZEN: annual spending (m.u.) on frozen products (Continuous)
(5) DETERGENTS_PAPER: annual spending (m.u.) on detergents and paper products (Continuous)
(6) DELICATESSEN: annual spending (m.u.) on and delicatessen products (Continuous);
(7) CHANNEL: customers Channel â€” Horeca (Hotel/Restaurant/Caf??) or Retail channel (Nominal)
(8) REGION: customers Region of Lisbon, Oporto or Other (Nominal)
For this study, we exclude the nominal attributes. Prepare a datatset (called org_data) with 6 continuous attributes.

```{r readData, include=TRUE}
# Import data
WholesaleData <- read.csv("~/R/DataMining/WholeSale/WholesaleCustomersData.csv")

# copy import data to new data frame
org_data <- WholesaleData

# remove the nominal attributes from the dataframe
org_data$Channel <- NULL
org_data$Region  <- NULL

```

## Data Exploration
Explore min, max, mean, standard deviation, correlation, and else using <b>describe</b> function.</br>
```{r describe, include=TRUE, echo=TRUE}
#Explore the data
describe(org_data)  
```

## Data Transformation I (Standardization)
- There is a lot of variation in the magnitude of the original data (org_data). To bring all the features to the same magnitude, standardize the features. 
- Also, showing first 10 rows in the transformed data (called trans_data).</br>
```{r transformation, include=TRUE, warning=FALSE}
#To standardize
library(BBmisc)

# Data 
# Normalized Data
trans_data <-
  normalize(
    org_data,
    method = "standardize",
    range = c(0, 1),
    margin = 1L,
    on.constant = "quiet"
  )
head(trans_data,10)

# Can also use the scale function to perform the same as normalize
# trans_data2 <- scale(org_data)
# head(trans_data2,10)
```

## Data Transformation II (Dimensionality Reduction)
- Conduct Principal Component Analysis (PCA) analysis to the trans_data. 
- Also, show first 10 data rows with principle components. 
```{r transaction1, include=TRUE}

# Find the covariance matrix S of the data.
S <- cov(trans_data[])

S # View the data

# The total variance Which is also equal to the sum of the eigenvalues of S
sum (diag(S))

s.eigen <- eigen(S)
s.eigen
```
- The percent of the total variance in the dataset the principle component 1 and principle component 2 account:
- The eigen-vectors represent the principal components of S. 
- The eigenvalues of S are used to find the proportion of the total variance explained by the components.

```{r transaction4, include=TRUE}
for (s in s.eigen$values) {
  print(s / sum(s.eigen$values))
}

```
- The first two principal components account for 72.45% (0.4408 + 0.2837) of the total variance.
- Generate a data set (called reduced_data) with 2 dimensions.</br>
```{r plot, include=TRUE}
trans_data.pca <- prcomp(trans_data[])

trans_data.pca

#The summary method of prcomp() also outputs the proportion of variance explained by the components.
summary(trans_data.pca)


scaling <- trans_data.pca$sdev[1:2] * sqrt(nrow(trans_data))
pc1 <- rowSums(t(t(sweep(trans_data[], 2 ,colMeans(trans_data[]))) * s.eigen$vectors[,1]) / scaling[1])
pc2 <- rowSums(t(t(sweep(trans_data[], 2, colMeans(trans_data[]))) * s.eigen$vectors[,2])*-1 / scaling[2])

reduced_data <- data.frame(pc1, pc2)
colnames(reduced_data) <- c('PC1', 'PC2')

# Data 
plot(s.eigen$values, xlab = 'Eigenvalue Number', ylab = 'Eigenvalue Size', main = 'Screen Graph')
lines(s.eigen$values)
```

## Data Visualization
- Plot original features with the principal components in the reduced_data with 2 features (dimensions).</brr>
- Plot 1 using the whole trans_data data set
- Plot 2 using the reduced_data set
```{r createDF,fig.width=10, include=TRUE}
library(ggfortify)
# Data create using prcomp() of R library
plot1 <- autoplot(trans_data.pca, data = trans_data, color= 'Group')

#Data create using eigenvalue
plot2 <- ggplot(reduced_data, aes(x=PC1, y=PC2)) + 
  geom_point()

#Add the 2 grid for comparison
library(gridExtra)
grid.arrange(plot1, plot2, nrow = 1)

```
(2) Can you find any cluster tendency visually: Yes</br>

## Cluster tendency
(1) Compute Hopkins statistic.</br>

(2) Determine whether the warehouse customer data shows useful clustering tendencies using the Hopkins statistic value.</br>

```{r clusterTendancy, fig.width=10, include=TRUE}
# 
plot1 <- autoplot(S, data = trans_data, color= 'Group')
plot1

```
## Optimal number of clusters (k)
Before the actual clustering, identify the optimal number of clusters (k) for the data with the trans_data data using (1) Elbow method and (2) Shilhouette method. For each result, draw the plot figure and give an optimal number of clusters with your reason.</br>

### Get_dist()
```{r OptimalCluster1, include=TRUE}
# The get distnc function()
library(factoextra)  # library for get_dist
distance <- get_dist(trans_data)
# Visualizing distance matrix
fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
```
### Elbow method
```{r elbowMethod, include=TRUE}

set.seed(123)
fviz_nbclust(trans_data, kmeans, method = "wss")
```
</br>
Optimal number of clusters: 10 </br>
Reason: It make sense to have 10 clusters according to the analyst.</br>

### Silhoute Method
```{r silhoute, include=TRUE}
fviz_nbclust(trans_data, kmeans, method = "silhouette")
```
</br>
Optimal number of clusters: 10 </br>
Reason: It make sense to have 10 clusters according to the analyst.</br>

## Representative-based clustering
Perform the cluster analysis using k-means with k=6. For the input data, use the trans_data data. 
(1) Report the representative of each cluster. 
```{r representativeBased, include=TRUE}
# Clustering
k2 <- kmeans(trans_data, centers = 6, nstart = 25)
str(k2)

fviz_cluster(k2, data = trans_data)
```

(2) Compute SSE of each cluster and total SSE of the clustering.</br>

```{r Sum of Square}
# Sum of Square of each Cluster
k2[["withinss"]]

# Total SSE
k2[["tot.withinss"]]
```


## Visualization of clusters I
Visualize the clustering result with the reduce_data with two features from PCA and the cluster label from the k-mean clustering. When you plot the data points, use different color per cluster.</br>
```{r Visualization, include=TRUE}
# Data 
 # Clustering
k2 <- kmeans(reduced_data, centers = 6, nstart = 25)
fviz_cluster(k2, data = reduced_data)

```

## Hierarchal clustering
(1) Perform the cluster analysis using a single link agglomerative hierarchical clustering algorithm. For the input data, use the trans_data data. Show the cluster dendogram.</br>
```{r singlelink, include=TRUE}
dist_mat <- dist(trans_data, method = 'euclidean')
hclust_ <- hclust(dist_mat, method = 'single')
plot(hclust_)

```
(2) Perform the cluster analysis using a complete link agglomerative hierarchical clustering algorithm. For the input data, use the trans_data data. Show the cluster dendogram.</br>
```{r completelink, include=TRUE}
hclust_ <- hclust(dist_mat, method = 'complete')
plot(hclust_)
```
(3) From the complete link hierarchical clustering result, report 6 clusters with their data points. And plot the clustering result with two features from PCA and the cluster label from the clustering result.</br>
```{r hierarchal, include=TRUE}
# Data 
cut_cluster <- cutree(hclust_, k = 6)
plot(cut_cluster)

suppressPackageStartupMessages(library(dendextend))
avg_dend_obj <- as.dendrogram(hclust_)
avg_col_dend <- color_branches(avg_dend_obj, h = 6)
plot(avg_col_dend)

# Plot the clustering result with two features from PCA and the cluster label from the clustering result.
suppressPackageStartupMessages(library(dplyr))

# Merge the cluster into reduce dataDBSCAN 
seeds_df_cl <- mutate(reduced_data, cluster = cut_cluster)

# count how many observations were assigned to each cluster with the count() function.
count(seeds_df_cl,cluster)

# Draw Blot
suppressPackageStartupMessages(library(ggplot2))
ggplot(seeds_df_cl, aes(x=PC1, y = PC2, color = factor(cluster))) + geom_point()

```

## Density-based clustering
Perform the cluster analysis using DBSCAN with ðœ– = 0.5 and ð‘šð‘–ð‘›ð‘ð‘¡ð‘  = 15. For the input data, use the trans_data data. Plot the clusters with two features from PCA and the cluster label from DBSCAN clustering results.</br>
```{r densityBased, include=TRUE}
# DBSCAN with eps= 0.5, MinPts = 15
dbscan_ <- dbscan(trans_data, eps= 0.5, MinPts = 15)
dbscan_cluster <-dbscan_[["cluster"]]

# Plot the clusters with two features from PCA
seeds_df_cl <- mutate(reduced_data, cluster = dbscan_cluster)

# count how many observations were assigned to each cluster with the count() function.
count(seeds_df_cl,cluster)

# Draw plot
ggplot(seeds_df_cl, aes(x=PC1, y = PC2, color = factor(cluster))) + geom_point()
```